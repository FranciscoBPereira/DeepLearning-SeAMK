{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/aR/0SNGx8XVpgDgGxbM2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranciscoBPereira/DeepLearning-SeAMK/blob/main/SeAMK2223_Ex2_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aofNbODMK2M"
      },
      "outputs": [],
      "source": [
        "# Setup, Version check and Common imports\n",
        "\n",
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 10)\n",
        "\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.10\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload file \"pt.txt\" to the working directory\n",
        "\n",
        "# Open file and add \"start\" and \"end\" to the target strings\n",
        "\n",
        "text_file = 'pt.txt'\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "or_text_pairs = []\n",
        "for line in lines:\n",
        "    english, port = line.split(\"\\t\")\n",
        "    port = \"[start] \" + port + \" [end]\"\n",
        "    or_text_pairs.append((english, port))"
      ],
      "metadata": {
        "id": "-DStQd8PL7c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a random sample of 10 examples\n",
        "\n",
        "import random\n",
        "for x in range(10):\n",
        "  print(x+1 , \": \", random.choice(or_text_pairs))"
      ],
      "metadata": {
        "id": "H_EpLbwwL7hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To speed-up training, we can consider just a fraction of the original dataset\n",
        "# Performance will degrade with the lack of data\n",
        "\n",
        "# 80% for training and 20% for validation\n",
        "\n",
        "fraction = 0.15\n",
        "\n",
        "random.shuffle(or_text_pairs)\n",
        "\n",
        "slice = int(len(or_text_pairs)*0.15)\n",
        "\n",
        "text_pairs = or_text_pairs[ : slice\n",
        "                                                  ]\n",
        "num_val_samples = int(0.20 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:]\n",
        "\n",
        "print('Original: ', len(or_text_pairs))\n",
        "\n",
        "print('Sliced: ', len(text_pairs))\n",
        "\n",
        "print('Training: ', len(train_pairs))\n",
        "\n",
        "print('Validation: ', len(val_pairs))\n"
      ],
      "metadata": {
        "id": "r7OujUN_L7kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing, Vectorization and Pipelining (you can skip these details)\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "# Vectorization settings\n",
        "sequence_length = 20\n",
        "vocab_size = 5000\n",
        "\n",
        "# English Vectorization\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Portuguese Vectorization\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "# Get vocabulary\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_pt_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_pt_texts)\n",
        "\n",
        "# Create Dataset object for training (https://www.tensorflow.org/api_docs/python/tf/data)\n",
        "\n",
        "# Tuples (inputs, target)\n",
        "# Inputs: Dictionary with 2 keys: \"encoder_inputs\" and \"decoder_inputs\",\n",
        "# corresponding the sentence in portuguese and english\n",
        "# Target: Sentence in portuguese with a one position offset to the right\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def format_dataset(eng, pt):\n",
        "    eng = source_vectorization(eng)\n",
        "    pt = target_vectorization(pt)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"portuguese\": pt[:, :-1],\n",
        "    }, pt[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, pt_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    pt_texts = list(pt_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, pt_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "-bn_pbHqPnYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Seq2Seq Model\n",
        "\n",
        "# GRU-based encoder, with a bidirectional set of GRU cells\n",
        "# The encoder output is given to the decoder\n",
        "\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(\n",
        "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)\n",
        "\n",
        "# GRU-based decoder\n",
        "# Unidirectional GRU layer that receives the output of the encoder as the initial state\n",
        "# Final dense layer with softmax activation. It predicts the next word\n",
        "\n",
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"portuguese\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.TimeDistributed(layers.Dense(vocab_size, activation=\"softmax\"))(x)\n",
        "\n",
        "# Complete Model\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
      ],
      "metadata": {
        "id": "S3tUZzLQQCLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.summary()"
      ],
      "metadata": {
        "id": "hbUXkJQkL7nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and Train\n",
        "\n",
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "history = seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)\n",
        "\n"
      ],
      "metadata": {
        "id": "OsGm6tbgQbn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the evolution of the accuracy metrics\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "x = pd.DataFrame(history.history, columns = ['accuracy', 'val_accuracy'])\n",
        "x.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pq-DISd6Qjng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try translate a few English sentences to Portuguese\n",
        "\n",
        "pt_vocab = target_vectorization.get_vocabulary()\n",
        "pt_index_lookup = dict(zip(range(len(pt_vocab)), pt_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "        next_token_predictions = seq2seq_rnn.predict(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "        sampled_token = pt_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_tok\n",
        "        en == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "for _ in range(5):\n",
        "    input_sentence = input()\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence), '\\n')"
      ],
      "metadata": {
        "id": "e3xJaiciQkRC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}